MDP Framework  (45%)68/68
Initialising the state and defining state and action spaces

Feedback: 
The state and action space were correctly defined.

Encoding state

Feedback: 
Encoding function for creating state-input and state-action input encoding was correctly implemented.

Was this helpful to you?
Completing the requests function

Feedback: Request function was correctly implemented.
Defining the reward function

Feedback: 
The reward function correctly computes the rewards using the time matrix.

Was this helpful to you?
Defining step function that returns next state given the current state and current action

Feedback: 
The next-state function was correctly implemented.

Training the DQN Agent (40%)60/60
Epsilon-Decay Strategy

Feedback: The Epsilon-greedy strategy was correctly implemented, and the epsilon value is decaying with the number of episodes.
Replay memory and weights of Q-network are initialised.

Feedback: 
Network weights and memory was correctly initialized.

Training the DQN model

Feedback: The calculation were correctly implemented in the train model function.
Deriving reward and new state from step function

Feedback: 
Rewards and new state were correctly derived using reward & next-state function.

Q-values Tracking (10%)5/15
Plotting convergence graphs

Feedback: 
Convergence plot was provided, the model need more tuning you could try more layer units and more number of episodes to see a better convergence.

Was this helpful to you?
Goodness of DQN Agent Trained

Feedback: 
The actual total rewards should converge to some value > 1800.

Was this helpful to you?
Coding Guidelines (5%)7/7
Coding Guidelines - Good Practices

Feedback: 
Done appropriately.
