Feedback:
MDP Framework of Numerical Tic-Tac-Toe (50%)0/150
Feedback: You missed to implement functions required for MDP framework to train an agent.
Was this helpful to you?
Defining winning states

Feedback: You have not implemented is_winning function. 
Was this helpful to you?
Define step function that incorporates transition from a state and action to the next state and reward

Feedback: You have not implemented any of the required functions in env file.
Was this helpful to you?
Training the Agent (Q-Learning) (35%)105/105
Feedback: Overall good work. You have completed code properly but due to incomplete env file you were not able to run your code. 
Epsilon-Decay Strategy

Feedback: Epsilon decay strategy implemented correctly to pick up next action.
Specifying Hyperparameters

Feedback: Hyper parameters are initialised and used correctly. 
Initialising the environment

Feedback: env initialised properly at the beginning of each episode. 
Deriving reward and new state from step function

Feedback: Reward derivation and state transition code blocks are completed properly. 
Q-update equation

Feedback: Q update for terminal and non terminal states are done correctly. 
Q-values Tracking (10%)15/30
Feedback: You missed to define logically correct state for tracking agent convergence. 
Was this helpful to you?
Sample states are tracked for convergence

Feedback: You have not defined logically correct states for tracking agent convergence. One of the states contains number five two times which is logically incorrect. 
Was this helpful to you?
Coding Guidelines (5%)15/15
Feedback: You missed to implement MDP framework functions. I would highly recommend to going through sample solution once it is available in upGrad portal. Happy Learning. 
Coding Guidelines - Good Practices

Feedback: You have written good quality code which is easy to read and understand. 
